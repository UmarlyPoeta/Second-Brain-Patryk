#  Pandas - DataFrame.drop_duplicates() - Usuwanie duplikat贸w

##  Co to jest drop_duplicates()?

`drop_duplicates()` to funkcja, kt贸ra usuwa duplikaty wierszy z DataFrame. To jak sprztanie szafy - zostawiasz tylko jeden egzemplarz ka偶dej rzeczy! Ч

##  Podstawowa skadnia

```python
DataFrame.drop_duplicates(
    subset=None,      # kolumny do sprawdzenia
    keep='first',     # kt贸ry duplikat zachowa  
    inplace=False,    # czy zmieni oryginalny DF
    ignore_index=False # czy zresetowa indeks
)
```

##  Podstawowe przykady

```python
import pandas as pd
import numpy as np

# DataFrame z duplikatami
dane = pd.DataFrame({
    'imie': ['Anna', 'Jan', 'Anna', 'Maria', 'Jan', 'Piotr', 'Anna'],
    'wiek': [25, 30, 25, 35, 30, 40, 26],  # Anna ma r贸偶ne wieki!
    'miasto': ['Warszawa', 'Krak贸w', 'Warszawa', 'Gdask', 'Krak贸w', 'Pozna', 'Wrocaw']
})

print("Dane z duplikatami:")
print(dane)

# Znajd藕 duplikaty
print(f"\nDuplikaty (wszystkie kolumny):")
print(dane.duplicated())  # True = duplikat

# Usu duplikaty
bez_duplikatow = dane.drop_duplicates()
print(f"\nBez duplikat贸w (zachowaj pierwsze):")
print(bez_duplikatow)

# Zachowaj ostatni duplikat
ostatnie = dane.drop_duplicates(keep='last')
print(f"\nZachowaj ostatnie:")
print(ostatnie)

# Usu WSZYSTKIE duplikaty (nawet orygina)
zadne = dane.drop_duplicates(keep=False)
print(f"\nUsu wszystkie (nawet oryginay):")
print(zadne)
```

##  Praktyczne przypadki

###  Czyszczenie danych klient贸w

```python
# Symulacja bazy klient贸w z duplikatami
np.random.seed(42)

klienci_brudne = pd.DataFrame({
    'email': ['anna@email.com', 'jan@email.com', 'anna@email.com', 
              'maria@email.com', 'piotr@email.com', 'jan@email.com',
              'ewa@email.com', 'anna@email.com'],
    'imie': ['Anna', 'Jan', 'Anna', 'Maria', 'Piotr', 'Jan', 'Ewa', 'Anna'],
    'telefon': ['123456789', '987654321', '123456789', '555666777', 
                '111222333', '987654321', '444555666', '999888777'],  # Anna ma r贸偶ne tel.
    'data_rejestracji': pd.to_datetime(['2023-01-15', '2023-02-20', '2023-01-15',
                                       '2023-03-10', '2023-04-05', '2023-02-20',
                                       '2023-05-12', '2023-06-08'])
})

print("=== CZYSZCZENIE BAZY KLIENTW ===")
print("Brudne dane:")
print(klienci_brudne)

# Sprawd藕 duplikaty r贸偶nie
print(f"\nDuplikaty - wszystkie kolumny: {klienci_brudne.duplicated().sum()}")
print(f"Duplikaty - tylko email: {klienci_brudne.duplicated(subset=['email']).sum()}")
print(f"Duplikaty - email + imi: {klienci_brudne.duplicated(subset=['email', 'imie']).sum()}")

# Strategia: email to klucz unikalny
print(f"\nDuplikaty wedug email:")
email_dups = klienci_brudne[klienci_brudne.duplicated(subset=['email'], keep=False)]
print(email_dups.sort_values('email'))

# Usu duplikaty email, zachowaj najnowszy
klienci_czyste = klienci_brudne.sort_values('data_rejestracji').drop_duplicates(
    subset=['email'], keep='last'
)

print(f"\nPo czyszczeniu (najnowsze rejestracje):")
print(klienci_czyste.sort_values('email'))

print(f"\nPodsumowanie:")
print(f"Przed: {len(klienci_brudne)} rekord贸w")
print(f"Po: {len(klienci_czyste)} rekord贸w") 
print(f"Usunito: {len(klienci_brudne) - len(klienci_czyste)} duplikat贸w")
```

###  Dane transakcyjne

```python
# Symulacja bd贸w w systemie patnoci (podw贸jne transakcje)
transakcje = pd.DataFrame({
    'transaction_id': ['T001', 'T002', 'T001', 'T003', 'T004', 'T002', 'T005'],
    'user_id': ['U123', 'U456', 'U123', 'U789', 'U123', 'U456', 'U999'],
    'kwota': [100.00, 250.50, 100.00, 75.25, 300.00, 250.50, 150.75],
    'timestamp': pd.to_datetime(['2023-01-01 10:00:00', '2023-01-01 11:30:00',
                                '2023-01-01 10:00:01', '2023-01-01 12:15:00',
                                '2023-01-01 13:45:00', '2023-01-01 11:30:02',
                                '2023-01-01 14:20:00']),
    'status': ['SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS']
})

print("=== ANALIZA DUPLIKATW TRANSAKCJI ===")
print("Wszystkie transakcje:")
print(transakcje)

# Identyfikuj potencjalne duplikaty
print(f"\nDuplikaty transaction_id:")
tid_dups = transakcje[transakcje.duplicated(subset=['transaction_id'], keep=False)]
print(tid_dups.sort_values('transaction_id'))

# Analiza duplikat贸w per user + kwota (podejrzane)
user_amount_dups = transakcje[
    transakcje.duplicated(subset=['user_id', 'kwota'], keep=False)
]
print(f"\nPodejrzane: ten sam user + ta sama kwota:")
print(user_amount_dups.sort_values(['user_id', 'kwota', 'timestamp']))

# Strategia czyszczenia: usu duplikaty transaction_id (zachowaj pierwsze)
transakcje_czyste = transakcje.drop_duplicates(subset=['transaction_id'], keep='first')

print(f"\nPo usuniciu duplikat贸w transaction_id:")
print(transakcje_czyste.sort_values('timestamp'))

# Podsumowanie finansowe
suma_przed = transakcje['kwota'].sum()
suma_po = transakcje_czyste['kwota'].sum()
print(f"\nWpyw na finanse:")
print(f"Suma przed czyszczeniem: {suma_przed:.2f} PLN")
print(f"Suma po czyszczeniu: {suma_po:.2f} PLN")
print(f"Zaoszczdzono: {suma_przed - suma_po:.2f} PLN (duplikaty)")
```

###  Analiza produkt贸w e-commerce

```python
# Dane produkt贸w z r贸偶nych 藕r贸de (duplikaty z r贸偶nymi cenami)
produkty = pd.DataFrame({
    'nazwa': ['iPhone 13', 'Samsung Galaxy', 'iPhone 13', 'MacBook Pro',
              'iPad Air', 'Samsung Galaxy', 'MacBook Pro', 'iPhone 13'],
    'cena': [3999, 2499, 3899, 8999, 2799, 2599, 8999, 4199],
    'sklep': ['Apple Store', 'MediaMarkt', 'x-kom', 'Apple Store',
              'MediaMarkt', 'RTV Euro AGD', 'x-kom', 'MediaExpert'],
    'data_aktualizacji': pd.to_datetime(['2023-06-01', '2023-06-02', '2023-06-01',
                                        '2023-06-03', '2023-06-02', '2023-06-01',
                                        '2023-06-03', '2023-06-04'])
})

print("=== ANALIZA CEN PRODUKTW ===")
print("Wszystkie oferty:")
print(produkty.sort_values(['nazwa', 'cena']))

# Znajd藕 duplikaty nazw (r贸偶ne ceny!)
nazwa_dups = produkty[produkty.duplicated(subset=['nazwa'], keep=False)]
print(f"\nProdukty z wieloma ofertami:")
print(nazwa_dups.sort_values(['nazwa', 'cena']))

# Analiza cen per produkt
print(f"\nR贸偶nice cenowe:")
for nazwa in produkty['nazwa'].unique():
    product_offers = produkty[produkty['nazwa'] == nazwa]
    if len(product_offers) > 1:
        min_price = product_offers['cena'].min()
        max_price = product_offers['cena'].max()
        savings = max_price - min_price
        print(f"{nazwa:15}: {min_price:4.0f} - {max_price:4.0f} PLN (oszczdno: {savings:4.0f})")

# Strategia: zostaw najtasz ofert ka偶dego produktu
najtansze = produkty.loc[produkty.groupby('nazwa')['cena'].idxmin()]

print(f"\nNajtasze oferty:")
print(najtansze.sort_values('cena'))

# Alternatywnie: najnowsze ceny
najnowsze = produkty.sort_values('data_aktualizacji').drop_duplicates(
    subset=['nazwa'], keep='last'
)

print(f"\nNajnowsze ceny:")
print(najnowsze.sort_values('nazwa'))
```

##  Zaawansowane techniki

### 1锔 Duplikaty z tolerancj

```python
# Dane z maymi r贸偶nicami (np. bdy zaokrgle)
dane_numeryczne = pd.DataFrame({
    'id': ['A', 'B', 'A', 'C', 'B'],
    'warto': [10.00, 20.50, 10.01, 30.75, 20.49],  # mae r贸偶nice
    'kategoria': ['X', 'Y', 'X', 'Z', 'Y']
})

print("=== DUPLIKATY Z TOLERANCJ ===")
print("Dane z maymi r贸偶nicami:")
print(dane_numeryczne)

# Standardowy drop_duplicates nie znajdzie duplikat贸w
standard = dane_numeryczne.drop_duplicates()
print(f"\nStandardowo (bez zmian): {len(standard)} rekord贸w")

# Zaokrglij przed sprawdzeniem duplikat贸w
dane_rounded = dane_numeryczne.copy()
dane_rounded['warto_rounded'] = dane_rounded['warto'].round(0)

duplikaty_rounded = dane_rounded.drop_duplicates(subset=['id', 'warto_rounded', 'kategoria'])
print(f"\nPo zaokrgleniu: {len(duplikaty_rounded)} rekord贸w")
print(duplikaty_rounded.drop('warto_rounded', axis=1))
```

### 2锔 Analiza przed usuniciem

```python
def analyze_duplicates(df, subset=None):
    """Szczeg贸owa analiza duplikat贸w przed usuniciem"""
    
    print("=== ANALIZA DUPLIKATW ===")
    
    # Og贸lne info
    total_rows = len(df)
    duplicated_rows = df.duplicated(subset=subset).sum()
    unique_rows = total_rows - duplicated_rows
    
    print(f"Wszystkie wiersze: {total_rows}")
    print(f"Duplikaty: {duplicated_rows} ({duplicated_rows/total_rows:.1%})")
    print(f"Unikalne: {unique_rows} ({unique_rows/total_rows:.1%})")
    
    if duplicated_rows > 0:
        # Poka偶 przykady duplikat贸w
        print(f"\nPrzykady duplikat贸w:")
        dups = df[df.duplicated(subset=subset, keep=False)]
        if subset:
            print(dups.sort_values(subset).head(10))
        else:
            print(dups.head(10))
        
        # Ile grup duplikat贸w?
        if subset:
            groups = df.groupby(subset).size()
            dup_groups = groups[groups > 1]
            print(f"\nGrup duplikat贸w: {len(dup_groups)}")
            print(f"Najwiksza grupa: {dup_groups.max()} duplikat贸w")
    
    return {
        'total': total_rows,
        'duplicates': duplicated_rows,
        'unique': unique_rows,
        'dup_percentage': duplicated_rows/total_rows
    }

# Test na danych klient贸w
stats = analyze_duplicates(klienci_brudne, subset=['email'])
```

## 锔 Czste problemy

```python
print("=== CZSTE PROBLEMY ===")

# 1. Zapomnienie o subset
df_test = pd.DataFrame({
    'id': [1, 2, 1, 3],
    'name': ['A', 'B', 'A', 'C'],
    'value': [10, 20, 15, 30]  # r贸偶ne wartoci dla id=1
})

print("1. Problem z subset:")
print("Dane:")
print(df_test)

print(f"\nBez subset - nie znajdzie duplikat贸w:")
print(df_test.drop_duplicates())  # r贸偶ne wartoci -> nie duplikat

print(f"\nZ subset=['id'] - znajdzie:")  
print(df_test.drop_duplicates(subset=['id']))

# 2. Resetowanie indeksu
print(f"\n2. Problem z indeksem:")
df_with_gaps = df_test.drop_duplicates(subset=['id'])
print("Indeks z lukami:")
print(df_with_gaps)

print("Po reset_index:")
print(df_with_gaps.reset_index(drop=True))

# Lub od razu:
clean_df = df_test.drop_duplicates(subset=['id'], ignore_index=True)
print("Z ignore_index=True:")
print(clean_df)
```

##  Podsumowanie

`drop_duplicates()` to niezbdne narzdzie data cleaningu:

- Ч Usuwa identyczne wiersze z DataFrame
-  `subset=` okrela kolumny do por贸wnania
-  `keep='first'/'last'/False` kontroluje kt贸re duplikaty zachowa
-  `inplace=True` modyfikuje oryginalny DataFrame
-  `ignore_index=True` resetuje indeks automatycznie
- 锔 Zawsze analizuj duplikaty przed usuniciem
-  Mo偶e by czci wikszego procesu ETL

To jak Marie Kondo dla danych - zostaw tylko to, co potrzebne! Ч