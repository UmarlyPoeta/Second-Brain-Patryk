## ğŸ§¹ Data Cleaning - Missing Values

_Strategie radzenia z brakujÄ…cymi danymi_

---

### ğŸ“ Wprowadzenie do Missing Values

**BrakujÄ…ce dane** to jeden z najczÄ™stszych problemÃ³w w Data Science. NieprawidÅ‚owe podejÅ›cie moÅ¼e:

1. **ZmniejszyÄ‡ dokÅ‚adnoÅ›Ä‡** modeli
2. **WprowadziÄ‡ bias** w analizie
3. **OgraniczyÄ‡** wielkoÅ›Ä‡ datasetu
4. **WpÅ‚ynÄ…Ä‡ na interpretacjÄ™** wynikÃ³w

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import warnings
warnings.filterwarnings('ignore')
```

---

### ğŸ” Typy brakujÄ…cych danych

```python
# Generowanie przykÅ‚adowych danych z rÃ³Å¼nymi typami missing values
np.random.seed(42)
n_samples = 1000

# Podstawowe dane
data = {
    'age': np.random.normal(35, 10, n_samples),
    'income': np.random.lognormal(10, 0.5, n_samples),
    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),
    'experience': np.random.normal(10, 5, n_samples)
}

df = pd.DataFrame(data)
df['experience'] = np.maximum(0, df['experience'])  # DoÅ›wiadczenie >= 0

print("=== TYPY BRAKUJÄ„CYCH DANYCH ===")

# 1. MCAR (Missing Completely At Random)
# Losowe 5% brakujÄ…cych wartoÅ›ci
mcar_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)
df.loc[mcar_indices, 'age'] = np.nan
print(f"MCAR: {df['age'].isnull().sum()} brakujÄ…cych wartoÅ›ci wieku (losowo)")

# 2. MAR (Missing At Random)
# Starsi ludzie rzadziej podajÄ… dochÃ³d
prob_missing = 1 / (1 + np.exp(-(df['age'] - 40) / 5))
mar_mask = np.random.random(len(df)) < prob_missing * 0.3
df.loc[mar_mask, 'income'] = np.nan
print(f"MAR: {df['income'].isnull().sum()} brakujÄ…cych dochodÃ³w (zaleÅ¼y od wieku)")

# 3. MNAR (Missing Not At Random)
# Ludzie z niskim wyksztaÅ‚ceniem nie podajÄ… lat doÅ›wiadczenia
mnar_mask = (df['education'] == 'High School') & (np.random.random(len(df)) < 0.4)
df.loc[mnar_mask, 'experience'] = np.nan
print(f"MNAR: {df['experience'].isnull().sum()} brakujÄ…cego doÅ›wiadczenia (zwiÄ…zane z wyksztaÅ‚ceniem)")

# Analiza wzorcÃ³w
def analyze_missing_patterns(df):
    missing_data = df.isnull()
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # 1. Missing data heatmap
    sns.heatmap(missing_data, cbar=True, ax=axes[0], cmap='viridis')
    axes[0].set_title('Mapa brakujÄ…cych danych')
    
    # 2. Missing data bar plot
    missing_counts = missing_data.sum()
    missing_counts.plot(kind='bar', ax=axes[1])
    axes[1].set_title('Liczba brakujÄ…cych wartoÅ›ci')
    axes[1].tick_params(axis='x', rotation=45)
    
    # 3. Missing combinations
    missing_combinations = missing_data.sum(axis=1).value_counts().sort_index()
    missing_combinations.plot(kind='bar', ax=axes[2])
    axes[2].set_title('Kombinacje brakujÄ…cych wartoÅ›ci')
    axes[2].set_xlabel('Liczba brakujÄ…cych kolumn na wiersz')
    
    plt.tight_layout()
    plt.show()

analyze_missing_patterns(df)
```

---

### ğŸ—‘ï¸ Strategia 1: Usuwanie danych

```python
def deletion_strategies(df):
    """RÃ³Å¼ne strategie usuwania brakujÄ…cych danych"""
    
    print("=== STRATEGIE USUWANIA ===")
    print(f"Rozmiar oryginalny: {df.shape}")
    
    # 1. Listwise deletion (complete case analysis)
    df_listwise = df.dropna()
    print(f"Listwise deletion: {df_listwise.shape} (utracono {len(df) - len(df_listwise)} wierszy)")
    
    # 2. Pairwise deletion - analiza korelacji
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    print(f"\nKorelacje z pairwise deletion:")
    corr_pairwise = df[numeric_cols].corr()  # Automatycznie ignoruje NaN pairwise
    print(corr_pairwise.round(3))
    
    # 3. Threshold-based deletion
    # UsuÅ„ kolumny z >50% missing values
    missing_threshold = 0.5
    cols_to_keep = df.columns[df.isnull().mean() < missing_threshold]
    df_col_filtered = df[cols_to_keep]
    print(f"\nPo usuniÄ™ciu kolumn >50% missing: {df_col_filtered.shape}")
    
    # UsuÅ„ wiersze z >50% missing values
    row_missing_pct = df.isnull().sum(axis=1) / df.shape[1]
    df_row_filtered = df[row_missing_pct < missing_threshold]
    print(f"Po usuniÄ™ciu wierszy >50% missing: {df_row_filtered.shape}")
    
    # 4. Conditional deletion
    # UsuÅ„ tylko jeÅ›li konkretna kolumna ma missing value
    df_conditional = df.dropna(subset=['income'])  # UsuÅ„ tylko jeÅ›li brak dochodu
    print(f"Conditional deletion (income): {df_conditional.shape}")
    
    return {
        'listwise': df_listwise,
        'column_filtered': df_col_filtered, 
        'row_filtered': df_row_filtered,
        'conditional': df_conditional
    }

deletion_results = deletion_strategies(df)

# PorÃ³wnanie rozkÅ‚adÃ³w po deletion
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

for i, (name, data) in enumerate(deletion_results.items()):
    ax = axes[i//2, i%2]
    if 'age' in data.columns and not data['age'].empty:
        data['age'].hist(bins=20, alpha=0.7, ax=ax)
        ax.set_title(f'RozkÅ‚ad wieku - {name}')
        ax.set_xlabel('Wiek')

plt.tight_layout()
plt.show()
```

---

### ğŸ”„ Strategia 2: Imputacja podstawowa

```python
def basic_imputation(df):
    """Podstawowe metody imputacji"""
    
    print("=== IMPUTACJA PODSTAWOWA ===")
    
    # Kopia danych do testÃ³w
    df_test = df.copy()
    
    # 1. Mean/Median/Mode imputation
    numeric_cols = df_test.select_dtypes(include=[np.number]).columns
    categorical_cols = df_test.select_dtypes(include=['object']).columns
    
    # Numeryczne - median (odporny na outliers)
    median_imputer = SimpleImputer(strategy='median')
    df_test[numeric_cols] = median_imputer.fit_transform(df_test[numeric_cols])
    
    # Kategoryczne - most frequent
    mode_imputer = SimpleImputer(strategy='most_frequent')
    df_test[categorical_cols] = mode_imputer.fit_transform(df_test[categorical_cols])
    
    print(f"Po imputacji podstawowej: {df_test.isnull().sum().sum()} brakujÄ…cych wartoÅ›ci")
    
    # 2. Forward/Backward fill (dla danych czasowych)
    df_ffill = df.fillna(method='ffill')  # Forward fill
    df_bfill = df.fillna(method='bfill')  # Backward fill
    
    # 3. Interpolacja
    df_interpolated = df.copy()
    for col in numeric_cols:
        df_interpolated[col] = df_interpolated[col].interpolate()
    
    # 4. Custom values
    df_custom = df.copy()
    custom_values = {
        'age': df['age'].median(),
        'income': 0,  # ZaÅ‚oÅ¼enie: brak dochodu = 0
        'experience': df['experience'].mean(),
        'education': 'Unknown'
    }
    
    df_custom = df_custom.fillna(value=custom_values)
    
    # PorÃ³wnanie wynikÃ³w
    imputation_comparison = pd.DataFrame({
        'Original': df.isnull().sum(),
        'Basic': df_test.isnull().sum(),
        'Forward Fill': df_ffill.isnull().sum(),
        'Interpolated': df_interpolated.isnull().sum(),
        'Custom': df_custom.isnull().sum()
    })
    
    print("\nPorÃ³wnanie metod imputacji:")
    print(imputation_comparison)
    
    return {
        'basic': df_test,
        'ffill': df_ffill,
        'interpolated': df_interpolated,
        'custom': df_custom
    }

basic_results = basic_imputation(df)
```

---

### ğŸ¤– Strategia 3: Imputacja zaawansowana

```python
def advanced_imputation(df):
    """Zaawansowane metody imputacji"""
    
    print("=== IMPUTACJA ZAAWANSOWANA ===")
    
    # Przygotowanie danych - kodowanie kategorycznych
    df_encoded = df.copy()
    df_encoded['education'] = pd.Categorical(df_encoded['education']).codes
    
    # 1. KNN Imputation
    knn_imputer = KNNImputer(n_neighbors=5)
    df_knn = pd.DataFrame(
        knn_imputer.fit_transform(df_encoded),
        columns=df_encoded.columns,
        index=df_encoded.index
    )
    
    # Dekodowanie education
    education_mapping = dict(enumerate(df['education'].astype('category').cat.categories))
    df_knn['education'] = df_knn['education'].round().astype(int).map(education_mapping)
    
    print(f"KNN Imputation: {df_knn.isnull().sum().sum()} brakujÄ…cych wartoÅ›ci")
    
    # 2. Iterative Imputation (MICE)
    iterative_imputer = IterativeImputer(random_state=42, max_iter=10)
    df_mice = pd.DataFrame(
        iterative_imputer.fit_transform(df_encoded),
        columns=df_encoded.columns,
        index=df_encoded.index
    )
    
    df_mice['education'] = df_mice['education'].round().astype(int).map(education_mapping)
    print(f"MICE Imputation: {df_mice.isnull().sum().sum()} brakujÄ…cych wartoÅ›ci")
    
    # 3. Group-based imputation
    df_group = df.copy()
    
    # Imputacja wieku na podstawie wyksztaÅ‚cenia
    age_by_education = df.groupby('education')['age'].median()
    for education in age_by_education.index:
        mask = (df_group['education'] == education) & df_group['age'].isnull()
        df_group.loc[mask, 'age'] = age_by_education[education]
    
    # Imputacja dochodu na podstawie wieku i wyksztaÅ‚cenia
    for education in df['education'].unique():
        education_data = df_group[df_group['education'] == education]
        if not education_data.empty:
            income_median = education_data['income'].median()
            mask = (df_group['education'] == education) & df_group['income'].isnull()
            df_group.loc[mask, 'income'] = income_median
    
    # Imputacja doÅ›wiadczenia na podstawie wieku
    mask = df_group['experience'].isnull()
    df_group.loc[mask, 'experience'] = np.maximum(0, df_group.loc[mask, 'age'] - 25)
    
    print(f"Group-based Imputation: {df_group.isnull().sum().sum()} brakujÄ…cych wartoÅ›ci")
    
    return {
        'knn': df_knn,
        'mice': df_mice,
        'group': df_group
    }

advanced_results = advanced_imputation(df)

# PorÃ³wnanie rozkÅ‚adÃ³w
def compare_distributions(original, imputed_dict, column):
    """PorÃ³wnanie rozkÅ‚adÃ³w przed i po imputacji"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    # Oryginalny rozkÅ‚ad (bez NaN)
    original_clean = original[column].dropna()
    original_clean.hist(bins=30, alpha=0.7, ax=axes[0], color='blue')
    axes[0].set_title(f'Oryginalny {column}')
    axes[0].axvline(original_clean.mean(), color='red', linestyle='--', label='Mean')
    axes[0].legend()
    
    # Imputowane rozkÅ‚ady
    for i, (name, data) in enumerate(imputed_dict.items(), 1):
        if i < len(axes):
            data[column].hist(bins=30, alpha=0.7, ax=axes[i])
            axes[i].set_title(f'{name.upper()} - {column}')
            axes[i].axvline(data[column].mean(), color='red', linestyle='--', label='Mean')
            axes[i].legend()
    
    plt.tight_layout()
    plt.show()

# PorÃ³wnanie dla wieku
all_methods = {**basic_results, **advanced_results}
compare_distributions(df, all_methods, 'age')
```

---

### ğŸ“Š Walidacja imputacji

```python
def validate_imputation(original, imputed_dict):
    """Walidacja jakoÅ›ci imputacji"""
    
    print("=== WALIDACJA IMPUTACJI ===")
    
    validation_results = []
    
    for method_name, imputed_df in imputed_dict.items():
        for column in original.columns:
            if original[column].dtype in [np.number]:
                # PorÃ³wnanie statystyk
                original_stats = original[column].describe()
                imputed_stats = imputed_df[column].describe()
                
                # Obliczenie rÃ³Å¼nic
                mean_diff = abs(imputed_stats['mean'] - original_stats['mean'])
                std_diff = abs(imputed_stats['std'] - original_stats['std'])
                
                # Test Kolmogorov-Smirnov
                from scipy.stats import ks_2samp
                original_clean = original[column].dropna()
                ks_stat, ks_pvalue = ks_2samp(original_clean, imputed_df[column])
                
                validation_results.append({
                    'Method': method_name,
                    'Column': column,
                    'Mean_Diff': mean_diff,
                    'Std_Diff': std_diff,
                    'KS_Statistic': ks_stat,
                    'KS_P_Value': ks_pvalue,
                    'Distribution_Similar': ks_pvalue > 0.05
                })
    
    validation_df = pd.DataFrame(validation_results)
    
    print("\nStatystyki walidacji:")
    print(validation_df.round(4))
    
    # Wizualizacja
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # Mean differences
    pivot_mean = validation_df.pivot(index='Method', columns='Column', values='Mean_Diff')
    sns.heatmap(pivot_mean, annot=True, ax=axes[0], cmap='Reds')
    axes[0].set_title('RÃ³Å¼nica Å›rednich')
    
    # Std differences  
    pivot_std = validation_df.pivot(index='Method', columns='Column', values='Std_Diff')
    sns.heatmap(pivot_std, annot=True, ax=axes[1], cmap='Reds')
    axes[1].set_title('RÃ³Å¼nica odchyleÅ„ standardowych')
    
    # KS test p-values
    pivot_ks = validation_df.pivot(index='Method', columns='Column', values='KS_P_Value')
    sns.heatmap(pivot_ks, annot=True, ax=axes[2], cmap='RdYlGn', vmin=0, vmax=1)
    axes[2].set_title('K-S Test P-Values (>0.05 = good)')
    
    plt.tight_layout()
    plt.show()
    
    return validation_df

# Walidacja wszystkich metod
validation_results = validate_imputation(df, all_methods)
```

---

### ğŸ¯ Strategia wyboru metody

```python
def imputation_strategy_guide():
    """Przewodnik wyboru strategii imputacji"""
    
    strategy_guide = """
    ğŸ¯ PRZEWODNIK WYBORU STRATEGII IMPUTACJI
    
    ğŸ“Š WEDÅUG TYPU DANYCH:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Typ danych      â”‚ Zalecana metoda                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Numeryczne      â”‚ Median, KNN, MICE                    â”‚
    â”‚ Kategoryczne    â”‚ Mode, KNN z encoding                 â”‚
    â”‚ Czasowe         â”‚ Forward/Backward fill, interpolacja  â”‚
    â”‚ Tekstowe        â”‚ Mode, "Unknown", domain-specific     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ğŸ“ˆ WEDÅUG PROCENTU MISSING:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Procent missing â”‚ Strategia                            â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ < 5%            â”‚ Listwise deletion, Mean/Median       â”‚
    â”‚ 5-15%           â”‚ KNN, MICE, Group-based               â”‚
    â”‚ 15-40%          â”‚ Advanced methods, Domain knowledge   â”‚
    â”‚ > 40%           â”‚ Consider dropping, Create indicator  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ğŸ” WEDÅUG WZORCA MISSING:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Typ missing     â”‚ PodejÅ›cie                            â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ MCAR            â”‚ Any imputation method                â”‚
    â”‚ MAR             â”‚ Model-based (KNN, MICE)              â”‚
    â”‚ MNAR            â”‚ Domain knowledge, Create indicators  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    âš ï¸  UWAGI:
    â€¢ Zawsze zachowaj oryginalny dataset
    â€¢ Dokumentuj wszystkie transformacje
    â€¢ Waliduj wpÅ‚yw na model
    â€¢ RozwaÅ¼ utworzenie wskaÅºnika missing
    â€¢ Testuj stabilnoÅ›Ä‡ imputacji
    """
    
    print(strategy_guide)
    
    # PrzykÅ‚ad decision tree dla wyboru metody
    def suggest_imputation_method(column_data, missing_pct):
        """Sugestia metody imputacji na podstawie danych"""
        
        if missing_pct > 40:
            return "Consider dropping or creating missing indicator"
        elif missing_pct < 5:
            return "Simple deletion or mean/median imputation"
        elif column_data.dtype in [np.number]:
            if missing_pct < 15:
                return "KNN or MICE imputation"
            else:
                return "Group-based or domain-specific imputation"
        else:  # categorical
            return "Mode or group-based imputation"
    
    # Sugestie dla naszego datasetu
    print("\nğŸ”§ SUGESTIE DLA OBECNEGO DATASETU:")
    for column in df.columns:
        missing_pct = (df[column].isnull().sum() / len(df)) * 100
        suggestion = suggest_imputation_method(df[column], missing_pct)
        print(f"â€¢ {column} ({missing_pct:.1f}% missing): {suggestion}")

imputation_strategy_guide()
```

---

### ğŸ’¡ Best Practices

```python
def imputation_best_practices():
    """Najlepsze praktyki w imputacji"""
    
    best_practices = """
    âœ… NAJLEPSZE PRAKTYKI IMPUTACJI
    
    ğŸ“‹ PRZED IMPUTACJÄ„:
    1. Zrozum przyczyny brakujÄ…cych danych
    2. Przeanalizuj wzorce missing data
    3. OkreÅ›l typ missing data (MCAR/MAR/MNAR)
    4. Zachowaj kopiÄ™ oryginalnych danych
    5. Udokumentuj wszystkie zaÅ‚oÅ¼enia
    
    ğŸ”„ PODCZAS IMPUTACJI:
    1. Testuj rÃ³Å¼ne metody
    2. Waliduj zachowanie rozkÅ‚adÃ³w
    3. SprawdÅº wpÅ‚yw na korelacje
    4. UÅ¼ywaj cross-validation
    5. Monitoruj performance modelu
    
    ğŸ“Š PO IMPUTACJI:
    1. PorÃ³wnaj statystyki przed/po
    2. SprawdÅº rozkÅ‚ady zmiennych
    3. Testuj stabilnoÅ›Ä‡ wynikÃ³w
    4. Dokumentuj wpÅ‚yw na model
    5. RozwaÅ¼ sensitivity analysis
    
    âš ï¸  CZÄ˜STE BÅÄ˜DY:
    â€¢ Imputacja przed train/test split
    â€¢ UÅ¼ywanie target variable w imputacji
    â€¢ Ignorowanie typu missing data
    â€¢ Brak walidacji wynikÃ³w
    â€¢ Over-imputation (>40% missing)
    """
    
    print(best_practices)

imputation_best_practices()
```

---

### ğŸ¯ NastÄ™pny krok

Poznasz **Data Cleaning - Duplicate Handling**:

- Identyfikacja duplikatÃ³w
- RÃ³Å¼ne typy duplikatÃ³w
- Strategie usuwania
- Fuzzy matching
- Validation after deduplication